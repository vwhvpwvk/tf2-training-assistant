{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa5d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/guide/gpu\n",
    "BATCH_SIZE = 1024\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcf77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile( \n",
    "                optimizer=tfa.optimizers.AdamW(learning_rate = 1e-3, \n",
    "                               weight_decay = 1e-5),\n",
    "            #   tf.keras.optimizers.SGD(learning_rate=1e-3),#, weight_decay = 1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd978d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, \n",
    "            train_labels, \n",
    "            batch_size = BATCH_SIZE,\n",
    "             validation_data=(test_images, test_labels),\n",
    "            epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2796fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ccfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning with optuna\n",
    "\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "\n",
    "x_train = train_images\n",
    "y_train = train_labels\n",
    "x_test = test_images\n",
    "y_test = test_labels\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "def create_model(learning_rate, weight_decay):\n",
    "    # regularizer = tf.keras.regularizers.l2(weight_decay)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'), #, kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax' ) # kernel_regularizer=regularizer)\n",
    "    ])\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate= learning_rate,\n",
    "                                    weight_decay = weight_decay )\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1) ## Use suggest_float(..., log=True) instead ##\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3) #adjust ranges as needed.\n",
    "    model = create_model(learning_rate, weight_decay)\n",
    "    model.fit(x_train, y_train, epochs=3, validation_split=0.2, verbose=0)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20) # Increase n_trials for more comprehensive search\n",
    "\n",
    "print('Number of finished trials: ', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value: ', trial.value)\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n",
    "\n",
    "best_learning_rate = study.best_params['learning_rate']\n",
    "best_weight_decay = study.best_params['weight_decay']\n",
    "\n",
    "# Retrain with the best learning rate and weight decay.\n",
    "best_model = create_model(best_learning_rate, best_weight_decay)\n",
    "best_model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
    "final_loss, final_accuracy = best_model.evaluate(x_test,y_test)\n",
    "\n",
    "print(f\"Final Test Accuracy with best LR and WD: {final_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "\n",
    "x_train = train_images\n",
    "y_train = train_labels\n",
    "x_test = test_images\n",
    "y_test = test_labels\n",
    "\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "def create_model(learning_rate, weight_decay):\n",
    "    # regularizer = tf.keras.regularizers.l2(weight_decay)\n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'), #, kernel_regularizer=regularizer),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax' ) # kernel_regularizer=regularizer)\n",
    "    ])\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate= learning_rate,\n",
    "                                    weight_decay = weight_decay )\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 10) ## Use suggest_float(..., log=True) instead ##\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2) #adjust ranges as needed.\n",
    "    model = create_model(learning_rate, weight_decay)\n",
    "    model.fit(x_train, y_train, epochs=3, validation_split=0.2, verbose=0, batch_size = BATCH_SIZE)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=30) # Increase n_trials for more comprehensive search\n",
    "\n",
    "print('Number of finished trials: ', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('  Value: ', trial.value)\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))\n",
    "\n",
    "best_learning_rate = study.best_params['learning_rate']\n",
    "best_weight_decay = study.best_params['weight_decay']\n",
    "\n",
    "# Retrain with the best learning rate and weight decay.\n",
    "best_model = create_model(best_learning_rate, best_weight_decay)\n",
    "best_model.fit(x_train, \n",
    "y_train, \n",
    "epochs=10, \n",
    "validation_split=0.2,\n",
    "batch_size = BATCH_SIZE)\n",
    "final_loss, final_accuracy = best_model.evaluate(x_test,y_test)\n",
    "\n",
    "print(f\"Final Test Accuracy with best LR and WD: {final_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839258ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrain with the best learning rate and weight decay.\n",
    "best_model = create_model(best_learning_rate, best_weight_decay)\n",
    "best_model.fit(x_train, \n",
    "y_train, \n",
    "epochs=300, \n",
    "validation_split=0.2,\n",
    "batch_size = BATCH_SIZE)\n",
    "final_loss, final_accuracy = best_model.evaluate(x_test,y_test)\n",
    "\n",
    "print(f\"Final Test Accuracy with best LR and WD: {final_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb208d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate, best_weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c03f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-2.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
