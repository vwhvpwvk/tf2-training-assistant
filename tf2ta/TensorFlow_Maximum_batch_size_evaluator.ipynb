{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1TknxTydPSL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_FzVwFyl6ug",
        "outputId": "3d6974a0-6449-497f-82b4-ac80b331dd87"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n",
            "WARNING:root:Exponential search completed 16 doublings without OOM. Max tested batch size: 32768. GPU memory might be very large or model/data small.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def check_gpu():\n",
        "    \"\"\"Checks for available GPUs and prints info.\"\"\"\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        logging.info(f\"Found GPU(s): {len(gpus)}\")\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logging.info(f\"  {gpu.name} - Memory Growth enabled.\")\n",
        "        return True\n",
        "    else:\n",
        "        logging.warning(\"No GPU found. Running on CPU. Batch size limit might be very high (system RAM).\")\n",
        "        return False\n",
        "\n",
        "def try_batch_size(model, optimizer, loss_fn, input_shape, output_shape, batch_size, dtype=tf.float32):\n",
        "    \"\"\"\n",
        "    Attempts a single training step with the given batch size.\n",
        "\n",
        "    Args:\n",
        "        model: The tf.keras.Model to test.\n",
        "        optimizer: The tf.keras.optimizers.Optimizer.\n",
        "        loss_fn: The tf.keras.losses.Loss function.\n",
        "        input_shape: Shape of a single input sample (excluding batch dimension).\n",
        "        output_shape: Shape of a single output sample (excluding batch dimension).\n",
        "        batch_size: The batch size to attempt.\n",
        "        dtype: Data type for tensors (e.g., tf.float32, tf.float16).\n",
        "\n",
        "    Returns:\n",
        "        True if the batch size works, False if an OOM error occurs.\n",
        "    \"\"\"\n",
        "    tf.keras.backend.clear_session() # Clear previous graphs/ops\n",
        "    gc.collect() # Force garbage collection\n",
        "\n",
        "    # Add batch dimension to shapes\n",
        "    batch_input_shape = (batch_size,) + input_shape\n",
        "    batch_output_shape = (batch_size,) + output_shape\n",
        "\n",
        "    # Generate dummy data for this batch size\n",
        "    # Using tf.zeros might be slightly faster and uses similar memory\n",
        "    try:\n",
        "        logging.debug(f\"Generating data for batch size {batch_size}...\")\n",
        "        # Use tf.Variable to ensure data stays on the target device (GPU)\n",
        "        x = tf.Variable(tf.zeros(batch_input_shape, dtype=dtype))\n",
        "        y_true = tf.Variable(tf.zeros(batch_output_shape, dtype=dtype))\n",
        "        logging.debug(\"Data generated.\")\n",
        "\n",
        "        logging.debug(f\"Attempting train step with batch size {batch_size}...\")\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Ensure model variables are created (important for first run)\n",
        "            if not model.built:\n",
        "               model.build(batch_input_shape)\n",
        "               logging.info(f\"Model built with input shape: {batch_input_shape}\")\n",
        "\n",
        "            y_pred = model(x, training=True)\n",
        "            loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Clean up tensors explicitly to help memory release\n",
        "        del x, y_true, y_pred, loss, grads\n",
        "        gc.collect()\n",
        "        logging.debug(f\"Batch size {batch_size} successful.\")\n",
        "        return True\n",
        "\n",
        "    except tf.errors.ResourceExhaustedError as e:\n",
        "        # Clean up tensors explicitly after OOM\n",
        "        del x, y_true # Other variables might not exist if OOM happened early\n",
        "        gc.collect()\n",
        "        logging.warning(f\"OOM error with batch size {batch_size}: {e}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "         # Clean up tensors explicitly after other errors\n",
        "        try:\n",
        "            del x, y_true, y_pred, loss, grads\n",
        "        except NameError:\n",
        "            pass # Ignore if variables weren't created before the error\n",
        "        gc.collect()\n",
        "        logging.error(f\"An unexpected error occurred with batch size {batch_size}: {e}\")\n",
        "        # Treat other errors like OOM for safety, as they might be memory related too\n",
        "        return False\n",
        "\n",
        "\n",
        "def find_max_batch_size(\n",
        "    model_builder,\n",
        "    optimizer_builder,\n",
        "    loss_fn_builder,\n",
        "    input_shape,\n",
        "    output_shape,\n",
        "    dtype=tf.float32,\n",
        "    initial_batch_size=1,\n",
        "    max_search_doubling=16 # Limit exponential search (2**(max_search_doubling)*initial)\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Finds the maximum batch size that fits in memory.\n",
        "\n",
        "    Args:\n",
        "        model_builder: A function that returns a new instance of the Keras model.\n",
        "        optimizer_builder: A function that returns a new instance of the optimizer.\n",
        "        loss_fn_builder: A function that returns a new instance of the loss function.\n",
        "        input_shape: Shape of a single input sample (excluding batch dimension).\n",
        "        output_shape: Shape of a single output sample (excluding batch dimension).\n",
        "        dtype: Data type for tensors (tf.float32, tf.float16).\n",
        "        initial_batch_size: The starting batch size for the search.\n",
        "        max_search_doubling: How many times to double the batch size in exponential search.\n",
        "\n",
        "    Returns:\n",
        "        The estimated maximum batch size, or 0 if even the initial size fails.\n",
        "    \"\"\"\n",
        "    check_gpu()\n",
        "    logging.info(f\"Starting search for max batch size. Input: {input_shape}, Output: {output_shape}, Dtype: {dtype}\")\n",
        "\n",
        "    batch_size = initial_batch_size\n",
        "    last_successful_batch_size = 0\n",
        "    oom_occurred = False\n",
        "    doubling_step = 0\n",
        "\n",
        "    # --- Exponential Search Phase ---\n",
        "    logging.info(\"--- Starting Exponential Search Phase ---\")\n",
        "    while doubling_step < max_search_doubling:\n",
        "        logging.info(f\"Trying batch size: {batch_size}\")\n",
        "        # Rebuild model and optimizer for clean state (important for memory)\n",
        "        model = model_builder()\n",
        "        optimizer = optimizer_builder()\n",
        "        loss_fn = loss_fn_builder()\n",
        "\n",
        "        # Special handling for first model build\n",
        "        if not model.built:\n",
        "           try:\n",
        "               model.build((batch_size,) + input_shape)\n",
        "               logging.info(f\"Model built with input shape: {(batch_size,) + input_shape}\")\n",
        "           except Exception as e:\n",
        "               logging.error(f\"Failed to build model even with batch size {batch_size}: {e}\")\n",
        "               return 0 # Cannot proceed if model build fails\n",
        "\n",
        "        if try_batch_size(model, optimizer, loss_fn, input_shape, output_shape, batch_size, dtype):\n",
        "            last_successful_batch_size = batch_size\n",
        "            batch_size *= 2 # Double for next attempt\n",
        "            doubling_step += 1\n",
        "            # Clean up before next iteration\n",
        "            del model, optimizer, loss_fn\n",
        "            gc.collect()\n",
        "        else:\n",
        "            oom_occurred = True\n",
        "            # Clean up the failed attempt's resources\n",
        "            del model, optimizer, loss_fn\n",
        "            gc.collect()\n",
        "            break # OOM occurred, move to binary search\n",
        "\n",
        "        # Add a small delay to allow GPU memory to stabilize if needed\n",
        "        time.sleep(0.5)\n",
        "\n",
        "\n",
        "    if not oom_occurred:\n",
        "        logging.warning(f\"Exponential search completed {max_search_doubling} doublings without OOM. Max tested batch size: {last_successful_batch_size}. GPU memory might be very large or model/data small.\")\n",
        "        # If it never failed, the last successful one is our best guess within the search limit\n",
        "        return last_successful_batch_size\n",
        "\n",
        "    # --- Binary Search Phase ---\n",
        "    logging.info(\"--- Starting Binary Search Phase ---\")\n",
        "    lower_bound = last_successful_batch_size\n",
        "    upper_bound = batch_size # This is the size that failed\n",
        "\n",
        "    # Ensure there's a gap to search\n",
        "    if lower_bound >= upper_bound -1 :\n",
        "         logging.info(f\"Binary search range too small ({lower_bound} to {upper_bound}). Result is {lower_bound}.\")\n",
        "         return lower_bound\n",
        "\n",
        "    logging.info(f\"Binary search range: ({lower_bound}, {upper_bound})\")\n",
        "\n",
        "    while lower_bound < upper_bound - 1:\n",
        "        test_batch_size = (lower_bound + upper_bound) // 2\n",
        "        if test_batch_size == lower_bound: # Avoid infinite loop if integer division doesn't progress\n",
        "             break\n",
        "\n",
        "        logging.info(f\"Trying batch size: {test_batch_size}\")\n",
        "\n",
        "        # Rebuild model and optimizer for clean state\n",
        "        model = model_builder()\n",
        "        optimizer = optimizer_builder()\n",
        "        loss_fn = loss_fn_builder()\n",
        "\n",
        "        if try_batch_size(model, optimizer, loss_fn, input_shape, output_shape, test_batch_size, dtype):\n",
        "            lower_bound = test_batch_size # This size worked, try higher\n",
        "            del model, optimizer, loss_fn\n",
        "            gc.collect()\n",
        "        else:\n",
        "            upper_bound = test_batch_size # This size failed, try lower\n",
        "            del model, optimizer, loss_fn\n",
        "            gc.collect()\n",
        "\n",
        "        time.sleep(0.5) # Optional delay\n",
        "\n",
        "    logging.info(f\"Binary search finished. Lower bound: {lower_bound}, Upper bound: {upper_bound}\")\n",
        "    return lower_bound # The largest size that succeeded\n",
        "\n",
        "# =======================================================\n",
        "# ===== USER CONFIGURATION ==============================\n",
        "# =======================================================\n",
        "\n",
        "# --- 1. Define Input/Output Shapes (excluding batch dimension) ---\n",
        "# Example for image classification (e.g., MNIST/CIFAR)\n",
        "# INPUT_SHAPE = (28, 28, 1) # Example: MNIST image shape\n",
        "# OUTPUT_SHAPE = (10,)       # Example: 10 classes (one-hot encoded expected by CategoricalCrossentropy)\n",
        "\n",
        "# Example for a simple MLP\n",
        "INPUT_SHAPE = (784,)      # Example: Flattened MNIST\n",
        "OUTPUT_SHAPE = (10,)       # Example: 10 classes\n",
        "\n",
        "# Example for sequence data (e.g., NLP)\n",
        "# INPUT_SHAPE = (128,)       # Example: Sequence length 128 (input IDs)\n",
        "# OUTPUT_SHAPE = (128, 50)   # Example: Sequence length 128, 50 output features per token\n",
        "\n",
        "# --- 2. Define Data Type ---\n",
        "# Use tf.float16 for mixed precision (requires compatible GPU and potentially loss scaling)\n",
        "# Use tf.float32 for standard precision\n",
        "DATA_TYPE = tf.float32\n",
        "\n",
        "# --- 3. Define Model Builder Function ---\n",
        "# Replace this with YOUR model architecture\n",
        "def build_my_model():\n",
        "    # Example: Simple MLP\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=INPUT_SHAPE), # Use InputLayer for clarity\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(OUTPUT_SHAPE[0], activation='softmax') # Adjust activation based on your task\n",
        "    ])\n",
        "    # Example: Simple CNN for images (use INPUT_SHAPE=(H, W, C))\n",
        "    # model = tf.keras.Sequential([\n",
        "    #     tf.keras.layers.InputLayer(input_shape=INPUT_SHAPE),\n",
        "    #     tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "    #     tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    #     tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "    #     tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    #     tf.keras.layers.Flatten(),\n",
        "    #     tf.keras.layers.Dropout(0.5),\n",
        "    #     tf.keras.layers.Dense(OUTPUT_SHAPE[0], activation=\"softmax\"),\n",
        "    # ])\n",
        "    return model\n",
        "\n",
        "# --- 4. Define Optimizer Builder Function ---\n",
        "def build_my_optimizer():\n",
        "    # Common choice: Adam\n",
        "    # return tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    # Another option: SGD\n",
        "     return tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
        "\n",
        "\n",
        "# --- 5. Define Loss Function Builder Function ---\n",
        "def build_my_loss_function():\n",
        "    # For classification with one-hot encoded labels\n",
        "    # return tf.keras.losses.CategoricalCrossentropy()\n",
        "    # For classification with integer labels\n",
        "    # return tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    # For regression\n",
        "    return tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "\n",
        "# --- 6. Set Initial Batch Size for Search ---\n",
        "# Start with 1 for safety, or a power of 2 you suspect might work\n",
        "INITIAL_BATCH_SIZE = 1\n",
        "\n",
        "# =======================================================\n",
        "# ===== EXECUTION =======================================\n",
        "# =======================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    max_bs = find_max_batch_size(\n",
        "        model_builder=build_my_model,\n",
        "        optimizer_builder=build_my_optimizer,\n",
        "        loss_fn_builder=build_my_loss_function,\n",
        "        input_shape=INPUT_SHAPE,\n",
        "        output_shape=OUTPUT_SHAPE,\n",
        "        dtype=DATA_TYPE,\n",
        "        initial_batch_size=INITIAL_BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    if max_bs > 0:\n",
        "        logging.info(f\"\\nEstimated Maximum Batch Size: {max_bs}\")\n",
        "        logging.info(\"Note: This is an estimate based on a single training step.\")\n",
        "        logging.info(\"Actual training might have slightly different memory requirements.\")\n",
        "        logging.info(f\"Consider using a slightly smaller batch size (e.g., {max_bs - (max_bs % 2 if max_bs > 1 else 0)}) for stability.\") # Suggest slightly smaller round number\n",
        "    else:\n",
        "        logging.error(\"\\nCould not find a working batch size, even the initial size failed.\")\n",
        "        logging.error(\"Check model complexity, input size, available GPU memory (use nvidia-smi), and data type (try float16 if applicable).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqG0f8lLl6uj"
      },
      "source": [
        "**How to Use:**\n",
        "\n",
        "1.  **Install TensorFlow:** If you haven't already: `pip install tensorflow` (or `tensorflow-gpu` if you manage CUDA manually, though the standard `tensorflow` package usually includes GPU support now).\n",
        "2.  **Configure:**\n",
        "    * Modify the `INPUT_SHAPE` and `OUTPUT_SHAPE` tuples to match *your* data (excluding the batch dimension).\n",
        "    * Set the `DATA_TYPE` (usually `tf.float32`, consider `tf.float16` for mixed-precision if your GPU supports it - check compatibility).\n",
        "    * Replace the example model architecture inside the `build_my_model` function with *your* actual model definition. **Crucially, ensure it returns a new model instance each time it's called.**\n",
        "    * Adjust `build_my_optimizer` and `build_my_loss_function` to return instances of the optimizer and loss you intend to use.\n",
        "    * You can optionally change `INITIAL_BATCH_SIZE`.\n",
        "3.  **Run:** Execute the Python script (`python your_script_name.py`).\n",
        "4.  **Observe:** The script will log its progress, trying increasing batch sizes until it hits an OOM error, then performing a binary search.\n",
        "5.  **Result:** The script will print the estimated maximum batch size found.\n",
        "\n",
        "**Important Considerations:**\n",
        "\n",
        "* **GPU Memory:** Ensure no other processes are consuming significant GPU memory while you run the script (check with `nvidia-smi` in your terminal if you have an NVIDIA GPU).\n",
        "* **Memory Fragmentation:** Sometimes, even if theoretically enough memory exists, fragmentation can prevent allocation. Restarting the Python kernel or even the machine can sometimes help.\n",
        "* **Mixed Precision (`tf.float16`):** Using `tf.float16` roughly halves the memory needed for activations and data but requires a GPU with Tensor Core support (Volta architecture or newer). You might also need to use loss scaling with your optimizer (`tf.keras.mixed_precision.LossScaleOptimizer`) for numerical stability during actual training.\n",
        "* **Model Complexity:** Larger models (more layers, wider layers, larger kernels) consume more memory.\n",
        "* **Estimate vs. Reality:** This script simulates one training step. Real training involves data loading pipelines, callbacks, validation steps, etc., which might slightly alter memory usage. It's often wise to use a batch size slightly *smaller* than the absolute maximum found for stability.\n",
        "* **CPU:** If run on a CPU, the limit will be system RAM, which is usually much less restrictive than GPU VRAM. The script will likely report a very large batch size."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
